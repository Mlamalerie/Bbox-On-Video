import streamlit as st
import cv2
import tempfile
import os
import time
from ultralytics import YOLO
import supervision as sv


@st.cache_resource
def load_model(model_path: str):
    """Charge et met en cache un mod√®le YOLO pour un chemin donn√©."""
    return YOLO(model_path)


MAX_DIRECT_SIDE = 1280


def create_box_annotator(style: str, thickness: int, palette: sv.ColorPalette | None):
    """Retourne un annotateur Supervision selon le style demand√© et la palette donn√©e."""

    color_kwargs = {"color": palette} if palette is not None else {}

    if style == "Rectangle":
        return sv.BoxAnnotator(thickness=thickness, **color_kwargs)
    if style == "Arrondi":
        return sv.RoundBoxAnnotator(thickness=thickness, **color_kwargs)
    if style == "Coins":
        return sv.BoxCornerAnnotator(thickness=thickness, **color_kwargs)
    if style == "Cercle":
        return sv.CircleAnnotator(thickness=thickness, **color_kwargs)
    if style == "Point":
        # DotAnnotator utilise un radius plut√¥t qu'une √©paisseur
        return sv.DotAnnotator(radius=max(1, thickness * 2), **color_kwargs)
    if style == "Ellipse":
        return sv.EllipseAnnotator(thickness=thickness, **color_kwargs)
    if style == "Trace":
        return sv.TraceAnnotator(thickness=thickness, **color_kwargs)
    # Fallback
    return sv.BoxAnnotator(thickness=thickness, **color_kwargs)


def run_detections(
    frame,
    model,
    conf_threshold: float,
    iou_threshold: float,
    use_sahi: bool,
):
    """Calcule les d√©tections sur une frame.

    - Si use_sahi est False ou que l'image est "petite", on appelle YOLO directement.
    - Sinon, on utilise sv.InferenceSlicer pour faire un d√©coupage type SAHI.
    """

    height, width = frame.shape[:2]

    if (not use_sahi) or max(height, width) <= MAX_DIRECT_SIDE:
        results = model(frame, conf=conf_threshold, iou=iou_threshold, verbose=False)[0]
        return sv.Detections.from_ultralytics(results)

    def callback(image_slice):
        results = model(
            image_slice,
            conf=conf_threshold,
            iou=iou_threshold,
            verbose=False,
        )[0]
        return sv.Detections.from_ultralytics(results)

    slicer = sv.InferenceSlicer(
        callback=callback,
        slice_wh=(768, 768),
        overlap_ratio_wh=(0.2, 0.2),
    )

    return slicer(image=frame)


st.set_page_config(page_title="Bbox Stories", layout="wide")

st.title("ÔøΩ Bbox Stories")
st.markdown(
    "*Glisse ta vid√©o. On la remixe en bo√Ætes, formes et couleurs.*"
)

# Sidebar pour la configuration
st.sidebar.header("Configuration")

# S√©lection du mod√®le
model_type = st.sidebar.radio(
    "Source du mod√®le",
    ("Mod√®les pr√©-entra√Æn√©s (YOLO11)", "Charger un mod√®le personnalis√© (.pt)")
)

model_path = None

if model_type == "Mod√®les pr√©-entra√Æn√©s (YOLO11)":
    selected_model = st.sidebar.selectbox(
        "Choisir un mod√®le YOLO11",
        ["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"],
    )
    model_path = selected_model
else:
    uploaded_model = st.sidebar.file_uploader("Charger votre fichier .pt", type=["pt"])
    if uploaded_model is not None:
        # Save uploaded model to a temp file so ultralytics can load it
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pt") as tmp_model:
            tmp_model.write(uploaded_model.read())
            model_path = tmp_model.name

# Param√®tres de d√©tection
st.sidebar.subheader("Param√®tres de d√©tection")

conf_options = [0.2, 0.3, 0.4, 0.5, 0.6]
iou_options = [0.3, 0.4, 0.5, 0.6, 0.7]

conf_threshold = st.sidebar.selectbox(
    "Seuil de confiance",
    options=conf_options,
    index=2,  # 0.4 par d√©faut
)

iou_threshold = st.sidebar.selectbox(
    "Seuil IOU",
    options=iou_options,
    index=2,  # 0.5 par d√©faut
)

# Style visuel des bbox
st.sidebar.subheader("Style des bbox")

bbox_thickness = st.sidebar.selectbox(
    "√âpaisseur des bbox",
    options=[1, 2, 4, 8, 16],
    index=1,  # 2 par d√©faut
)

bbox_style_map = {
    "‚¨õ Rectangle": "Rectangle",
    "üü¶ Arrondi": "Arrondi",
    "üìê Coins": "Coins",
    "‚ö™ Cercle": "Cercle",
    "üîπ Point": "Point",
    "üí† Ellipse": "Ellipse",
    "üåÄ Trace": "Trace",
}

selected_bbox_style_label = st.sidebar.selectbox(
    "Style de bbox",
    options=list(bbox_style_map.keys()),
    index=0,
)

bbox_style = bbox_style_map[selected_bbox_style_label]

palette_labels = [
    "Aucune",
    "üåà viridis",
    "üî• inferno",
    "üíú plasma",
    "üåã magma",
    "üåä cividis",
    "üçÉ Greens",
    "üü¶ cool",
    "üåÖ autumn",
    "üíõ Wistia",
    "üîÆ Purples",
]

palette_map = {
    "Aucune": None,
    "üåà viridis": "viridis",
    "üî• inferno": "inferno",
    "üíú plasma": "plasma",
    "üåã magma": "magma",
    "üåä cividis": "cividis",
    "üçÉ Greens": "Greens",
    "üü¶ cool": "cool",
    "üåÖ autumn": "autumn",
    "üíõ Wistia": "Wistia",
    "üîÆ Purples": "Purples",
}

palette_choice = st.sidebar.selectbox(
    "Palette de couleurs",
    options=palette_labels,
    index=0,
)

bbox_palette = None
mpl_palette_name = palette_map.get(palette_choice)
if mpl_palette_name is not None:
    # N = 16 couleurs distinctes par d√©faut
    bbox_palette = sv.ColorPalette.from_matplotlib(mpl_palette_name, 16)

# Labels
st.sidebar.subheader("Labels")

label_mode = st.sidebar.selectbox(
    "Affichage des labels",
    options=[
        "Aucun",
        "Label seulement",
        "Score seulement",
        "Label + score",
    ],
    index=3,
)

# "Puissances de 2" pour l'√©chelle de texte (labels)
if label_mode != "Aucun":
    label_scale = st.sidebar.selectbox(
        "Taille des labels",
        options=[0.25, 0.5, 1.0, 2.0],  # ~2^-2, 2^-1, 2^0, 2^1
        index=1,  # 0.5 par d√©faut
    )
else:
    # valeur par d√©faut utilis√©e mais non affich√©e
    label_scale = 0.5

# SAHI / grandes vid√©os
st.sidebar.subheader("SAHI / grandes vid√©os")

use_sahi = st.sidebar.checkbox(
    "Activer le d√©coupage (SAHI) pour les grandes images",
    value=True,
    help=(
        "Si l'image est plus grande qu'une certaine taille, elle est d√©coup√©e en tuiles "
        "pour la d√©tection, ce qui peut am√©liorer la d√©tection des petits objets."
    ),
)

# Upload vid√©o
uploaded_video = st.file_uploader("Choisissez une vid√©o", type=["mp4", "avi", "mov", "mkv"])

if uploaded_video and model_path:
    # Sauvegarde temporaire de la vid√©o upload√©e
    tfile = tempfile.NamedTemporaryFile(delete=False)
    tfile.write(uploaded_video.read())
    video_path = tfile.name

    # Nom de base d√©riv√© de la vid√©o d'entr√©e
    original_name = uploaded_video.name or "video"
    base_name, _ = os.path.splitext(original_name)

    st.video(video_path)

    # Chargement du mod√®le (partag√© entre aper√ßu et traitement complet)
    st.write("Chargement du mod√®le...")
    try:
        model = load_model(model_path)
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le: {e}")
        st.stop()

    # Aper√ßu sur plusieurs frames (d√©but, milieu, fin)
    try:
        video_info = sv.VideoInfo.from_video_path(video_path)
        total_frames = video_info.total_frames or 0

        if total_frames <= 0:
            raise ValueError("Nombre de frames invalide pour l'aper√ßu")

        indices = [0]
        if total_frames > 2:
            indices.append(total_frames // 2)
        if total_frames > 1:
            indices.append(total_frames - 1)

        cap = cv2.VideoCapture(video_path)
        preview_images = []  # list of (title, annotated_frame)

        def annotate_frame(frame):
            detections = run_detections(
                frame,
                model,
                conf_threshold=conf_threshold,
                iou_threshold=iou_threshold,
                use_sahi=use_sahi,
            )

            box_annot = create_box_annotator(bbox_style, bbox_thickness, bbox_palette)
            if bbox_palette is not None:
                label_annot = sv.LabelAnnotator(color=bbox_palette, text_scale=label_scale)
            else:
                label_annot = sv.LabelAnnotator(text_scale=label_scale)

            labels = []
            if detections.class_id is not None and detections.confidence is not None:
                for class_id, confidence in zip(detections.class_id, detections.confidence):
                    class_name = model.model.names[class_id]
                    if label_mode == "Label seulement":
                        labels.append(f"{class_name}")
                    elif label_mode == "Score seulement":
                        labels.append(f"{confidence:.2f}")
                    elif label_mode == "Label + score":
                        labels.append(f"{class_name} {confidence:.2f}")

            annotated = box_annot.annotate(scene=frame.copy(), detections=detections)
            if label_mode != "Aucun" and len(labels) > 0:
                annotated = label_annot.annotate(scene=annotated, detections=detections, labels=labels)

            return annotated

        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if not ret or frame is None:
                continue
            annotated = annotate_frame(frame)

            if idx == 0:
                title = "Premi√®re frame annot√©e"
            elif idx == indices[-1]:
                title = "Derni√®re frame annot√©e"
            else:
                title = "Frame du milieu annot√©e"

            preview_images.append((title, annotated))

        cap.release()

        if preview_images:
            st.subheader("Aper√ßu")
            cols = st.columns(len(preview_images))
            for col, (title, img) in zip(cols, preview_images):
                h, w = img.shape[:2]
                display_width = min(640, w)
                with col:
                    st.caption(title)
                    st.image(img[:, :, ::-1], width=display_width)
        else:
            st.warning("Impossible de g√©n√©rer l'aper√ßu : aucune frame lisible.")

    except Exception as e:
        st.warning(f"Impossible de g√©n√©rer l'aper√ßu : {e}")

    if st.button("Lancer la d√©tection"):
        st.write("Traitement en cours....")

        box_annotator = create_box_annotator(bbox_style, bbox_thickness, bbox_palette)
        if bbox_palette is not None:
            label_annotator = sv.LabelAnnotator(color=bbox_palette, text_scale=label_scale)
        else:
            label_annotator = sv.LabelAnnotator(text_scale=label_scale)

        video_info = sv.VideoInfo.from_video_path(video_path)
        frame_generator = sv.get_video_frames_generator(source_path=video_path)

        output_path = os.path.join(tempfile.gettempdir(), f"{base_name}_annotated.mp4")

        progress_bar = st.progress(0)
        status_text = st.empty()

        start_time = time.time()

        try:
            with sv.VideoSink(target_path=output_path, video_info=video_info) as sink:
                for frame_index, frame in enumerate(frame_generator):
                    if video_info.total_frames:
                        progress = min((frame_index + 1) / video_info.total_frames, 1.0)
                        progress_bar.progress(progress)

                    detections = run_detections(
                        frame,
                        model,
                        conf_threshold=conf_threshold,
                        iou_threshold=iou_threshold,
                        use_sahi=use_sahi,
                    )

                    annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections)

                    labels = []
                    if detections.class_id is not None and detections.confidence is not None:
                        for class_id, confidence in zip(detections.class_id, detections.confidence):
                            class_name = model.model.names[class_id]
                            if label_mode == "Label seulement":
                                labels.append(f"{class_name}")
                            elif label_mode == "Score seulement":
                                labels.append(f"{confidence:.2f}")
                            elif label_mode == "Label + score":
                                labels.append(f"{class_name} {confidence:.2f}")

                    if label_mode != "Aucun" and len(labels) > 0:
                        annotated_frame = label_annotator.annotate(
                            scene=annotated_frame,
                            detections=detections,
                            labels=labels,
                        )

                    sink.write_frame(frame=annotated_frame)

            # Re-encodage pour un MP4 plus compatible navigateur
            web_output_path = os.path.join(tempfile.gettempdir(), f"{base_name}_annotated_web.mp4")
            cap = cv2.VideoCapture(output_path)
            fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            writer = cv2.VideoWriter(web_output_path, fourcc, fps, (width, height))

            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                writer.write(frame)

            cap.release()
            writer.release()

            elapsed = time.time() - start_time
            progress_bar.progress(1.0)
            status_text.success(f"Traitement termin√© en {elapsed:.1f} secondes !")

            # Notification √©ph√©m√®re
            st.toast(f"Vid√©o trait√©e en {elapsed:.1f} secondes", icon="‚úÖ")

            st.subheader("R√©sultat")
            st.video(web_output_path)

            with open(web_output_path, "rb") as file:
                st.download_button(
                    label="T√©l√©charger la vid√©o annot√©e",
                    data=file,
                    file_name=f"{base_name}_annotated.mp4",
                    mime="video/mp4",
                )

        except Exception as e:
            st.error(f"Une erreur est survenue lors du traitement : {e}")
            
elif not model_path and uploaded_video:
    st.info("Veuillez s√©lectionner ou charger un mod√®le YOLO.")
elif model_path and not uploaded_video:
    st.info("Veuillez charger une vid√©o.")

